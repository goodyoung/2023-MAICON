{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install library\n",
    "# !pip install -q timm\n",
    "# !pip install -q albumentations\n",
    "# !pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from einops import rearrange\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import albumentations as A\n",
    "import torch.optim as optim\n",
    "from ultralytics import YOLO\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import graycomatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_co_occur_mat(img):\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    r_channel = img[:, :, 2]\n",
    "    g_channel = img[:, :, 1]\n",
    "    b_channel = img[:, :, 0]\n",
    "    \n",
    "    distance = [1]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    red_matrix = graycomatrix(r_channel, distances=distance, angles=angles, symmetric=True, normed=True)\n",
    "    green_matrix = graycomatrix(g_channel, distances=distance, angles=angles, symmetric=True, normed=True)\n",
    "    blue_matrix = graycomatrix(b_channel, distances=distance, angles=angles, symmetric=True, normed=True)\n",
    "    combined_matrix = np.concatenate([red_matrix, green_matrix, blue_matrix], axis=-1)\n",
    "    result = combined_matrix.squeeze(2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.effi = timm.create_model(\"efficientnet_b0\", pretrained=True)\n",
    "        self.effi.conv_stem = nn.Conv2d(15, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        self.effi.bn2 = nn.Conv2d(1280,64,kernel_size =1, stride =1)\n",
    "        self.effi.drop = nn.Identity()\n",
    "        self.effi.act = nn.Identity()\n",
    "        self.effi.global_pool = nn.Identity()\n",
    "        self.effi.classifier = nn.Flatten()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4096,1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1000, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.effi(x)\n",
    "        x = self.effi.global_pool(x)\n",
    "        x = self.effi.classifier(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, video_paths: list, labels: list=None, mode=\"train\", n=1, randomness=False):\n",
    "        assert mode in ['train', 'test', 'validation','val']\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.n = n\n",
    "        self.randomness = randomness\n",
    "        self.yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
    "        t = [A.Resize(480,480),\n",
    "             A.CenterCrop(256,256,p=1),\n",
    "             A.Normalize(0.5,0.5),\n",
    "             ToTensorV2()]\n",
    "        if mode == 'train':\n",
    "            t = [A.Resize(480,480),\n",
    "                 A.CenterCrop(256,256,p=1),\n",
    "                 A.HorizontalFlip(p=0.5),\n",
    "                 A.Normalize(0.5,0.5),\n",
    "                 ToTensorV2()]\n",
    "        self.transform = A.Compose(t)\n",
    "        self.crop = A.Compose([A.CenterCrop(180,180,p=1),\n",
    "                               A.Resize(256,256),\n",
    "                               A.Normalize(0.5,0.5),\n",
    "                               ToTensorV2()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def _get_co_occurrence_matrix(self,frames):\n",
    "        frames = torch.squeeze(frames, 0)# (3,256,256)\n",
    "\n",
    "        # 이미지를 그레이스케일로 변환\n",
    "        transform = transforms.Grayscale()\n",
    "        gray_image = transform(frames)\n",
    "\n",
    "        # PyTorch Tensor를 NumPy 배열로 변환하고 [0, 255] 범위로 스케일 조정\n",
    "        gray_image_np = (gray_image.numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        # Co-occurrence matrix 계산\n",
    "        distances = [1, 2, 3]  # 거리 설정\n",
    "        angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]  # 방향 설정\n",
    "\n",
    "        co_occurrence_matrices = []\n",
    "        for channel in range(gray_image_np.shape[0]):\n",
    "            co_occurrence_matrix = graycomatrix(gray_image_np[channel], distances=distances, angles=angles, levels=256, symmetric=True, normed=True)\n",
    "            co_occurrence_matrices.append(co_occurrence_matrix)\n",
    "\n",
    "        # 각 채널별로 계산된 co-occurrence matrix를 합치기\n",
    "        co_occurrence_matrix_combined = np.stack(co_occurrence_matrices, axis=0) # (1, 256, 256, 3, 4)\n",
    "        co_occurrence_matrix_combined = torch.from_numpy(co_occurrence_matrix_combined)\n",
    "        co_occurrence_matrix_combined = torch.squeeze(co_occurrence_matrix_combined, 0)\n",
    "        co_occurrence_matrix_combined = rearrange(co_occurrence_matrix_combined, 'w h x y -> (x y) w h') # (12,256,256)\n",
    "        co_occurrence_matrix_combined = torch.unsqueeze(co_occurrence_matrix_combined ,0)\n",
    "\n",
    "        return co_occurrence_matrix_combined\n",
    "    \n",
    "    def _get_video_frames(self, cap):\n",
    "        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frames = []\n",
    "        \n",
    "        trial = 0\n",
    "        while len(frames) < self.n:\n",
    "            if self.randomness:\n",
    "                move_to = random.randint(1, num_frames-10*self.n)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, move_to)\n",
    "            ret, frame = cap.read()\n",
    "            if ret: # 프레임 존재\n",
    "                results = self.yolo([frame])\n",
    "                df = results.pandas().xyxy[0]\n",
    "                df = df[df['name']=='person']\n",
    "                if len(df) >= 1:\n",
    "                    xmin, ymin, xmax, ymax, _,_,_ = df.iloc[0]\n",
    "                    xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
    "                    frame = frame[ymin:ymax,xmin:xmax,:]\n",
    "                    frame = self.transform(image=frame)['image']\n",
    "                    frames.append(frame)\n",
    "                else: # 프레임이 존재하나 욜로 모델이 검출을 못함\n",
    "                    trial += 1\n",
    "                    move_to = random.randint(1, num_frames-10*self.n)\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, move_to)\n",
    "                    if trial == 3:\n",
    "                        trial = 0\n",
    "                        ret, frame = cap.read()\n",
    "                        frame = self.crop(image=frame)['image']\n",
    "                        frames.append(frame)\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                trial += 1\n",
    "                move_to = random.randint(1, num_frames-10*self.n)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, move_to)\n",
    "                if trial == 3:\n",
    "                    trial = 0\n",
    "                    ret, frame = cap.read()\n",
    "                    frame = self.crop(image=frame)['image']\n",
    "                    frames.append(frame)\n",
    "                else:\n",
    "                    continue\n",
    "        frames = torch.stack(frames) # (1,3,256,256)\n",
    "        co = self._get_co_occurrence_matrix(frames)\n",
    "        frames = torch.cat([frames, co], dim=1)\n",
    "        frames = frames.detach().clone()\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        video_path = self.video_paths[index]\n",
    "        # 랜덤 프레임 가져오기\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = self._get_video_frames(cap)\n",
    "        cap.release()\n",
    "        if self.mode == 'test':\n",
    "            return frames\n",
    "        else:\n",
    "            return frames, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.train_path = '/mnt/elice/dataset/train'\n",
    "        self.test_path = '/mnt/elice/dataset/test'\n",
    "        self.submission_csv = \"./sample_submission_v0.csv\"\n",
    "        self.save_path = \"./best_effi3.pth\"\n",
    "        self.EPOCHS = 6\n",
    "        self.LR = 0.001\n",
    "        self.BATCH_SIZE = 16\n",
    "        self.MAX_NORM = 5\n",
    "        self.NUM_WORKERS = 0\n",
    "        self.NUM_CLASSES = 1\n",
    "        self.DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.SEED = 777\n",
    "\n",
    "    def setup(self):\n",
    "        \n",
    "        seed_everything(self.SEED)\n",
    "        # 재현성 위해 sorting\n",
    "        train_fakes = sorted(glob(f\"{self.train_path}/fake/*\"))\n",
    "        train_reals = sorted(glob(f\"{self.train_path}/real/*\"))\n",
    "        self.submit = pd.read_csv(self.submission_csv)\n",
    "        x_test = [os.path.join(self.test_path, path) for path in self.submit[\"path\"].values]\n",
    "        # test_video_paths = sorted(glob(f\"{self.train_path}/*\"))\n",
    "        # fake이면 1 real이면 0으로 할당\n",
    "        train_video_paths = train_fakes + train_reals\n",
    "        labels = [1 for _ in range(len(train_fakes))] + [0 for _ in range(len(train_reals))]\n",
    "        x_train, x_val, y_train, y_val = train_test_split(\n",
    "            train_video_paths,\n",
    "            labels,\n",
    "            test_size=0.2,\n",
    "            random_state=self.SEED\n",
    "        )\n",
    "        \n",
    "        train_dataset = CustomDataset(video_paths=x_train, labels=y_train, mode=\"train\")\n",
    "        val_dataset = CustomDataset(video_paths=x_val, labels=y_val, mode=\"val\")\n",
    "        test_dataset = CustomDataset(video_paths=x_test, labels=None, mode=\"test\")\n",
    "        \n",
    "        self.train_dataloader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=self.NUM_WORKERS,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        self.val_dataloader = DataLoader(\n",
    "            dataset=val_dataset, \n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        self.test_dataloader = DataLoader(\n",
    "            dataset=test_dataset, \n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        self.model = CustomModel()\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        self.model.to(self.DEVICE)\n",
    "        optimizer = optim.AdamW(params=self.model.parameters(), lr=self.LR, weight_decay=1e-3)\n",
    "        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            cooldown=5,\n",
    "            min_lr=1e-9,\n",
    "            threshold_mode='abs',\n",
    "        )\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        best_model = None\n",
    "        \n",
    "        for epoch in range(1, self.EPOCHS+1):\n",
    "            self.model.train()\n",
    "            train_losses = []\n",
    "            for imgs, labels in tqdm(self.train_dataloader):\n",
    "                imgs = torch.squeeze(imgs, 1)\n",
    "                imgs = imgs.float().to(self.DEVICE) # (b,3,h,w)\n",
    "                labels = labels.float().to(self.DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                # with torch.cuda.amp.autocast():\n",
    "                output = self.model(imgs)\n",
    "                output = output.squeeze(-1)\n",
    "                loss = self.loss_fn(output, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.MAX_NORM)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_losses.append(loss.item())\n",
    "                \n",
    "\n",
    "            val_loss, val_acc = self._valid()\n",
    "            train_loss = np.mean(train_losses)\n",
    "            print(f\"EPOCH: {epoch}, TRAIN LOSS: {train_loss:.4f}, VAL LOSS: {val_loss:.4f}, VAL ACC: {val_acc:.4f}\")\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(val_acc)\n",
    "\n",
    "            if best_val_acc <= val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_model = deepcopy(self.model)\n",
    "                # save best model\n",
    "                torch.save(self.model.state_dict(), self.save_path)\n",
    "                early_stop = 0\n",
    "            else:\n",
    "                early_stop += 1\n",
    "\n",
    "            if early_stop > 7:\n",
    "                break\n",
    "    \n",
    "    def _valid(self):\n",
    "        self.model.eval()\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.val_dataloader):\n",
    "                imgs = torch.squeeze(imgs, 1)\n",
    "                imgs = imgs.float().to(self.DEVICE)\n",
    "                labels = labels.float().to(self.DEVICE)\n",
    "                \n",
    "                probs = self.model(imgs)\n",
    "                probs = probs.squeeze(-1)\n",
    "                loss = self.loss_fn(probs, labels)\n",
    "                probs = probs.cpu().detach().numpy()\n",
    "                labels = labels.cpu().detach().numpy()\n",
    "\n",
    "                preds = probs > 0.5\n",
    "                batch_acc = (labels == preds).mean()\n",
    "                val_accs.append(batch_acc)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        return np.mean(val_losses), np.mean(val_accs)\n",
    "    \n",
    "    def test(self, threshold=0.5):\n",
    "        answer_lst = []\n",
    "        logit_lst = []\n",
    "        logit_df = deepcopy(self.submit)\n",
    "        # model load\n",
    "        model = CustomModel(self.NUM_CLASSES).to(self.DEVICE)\n",
    "        model.load_state_dict(torch.load(self.save_path))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for imgs in tqdm(self.test_dataloader):\n",
    "                imgs = torch.squeeze(imgs, 1)\n",
    "                imgs = imgs.float().to(self.DEVICE)\n",
    "\n",
    "                probs = model(imgs)\n",
    "                probs = probs.squeeze(-1)\n",
    "                probs = probs.cpu().detach().numpy()\n",
    "                logit_lst.append(probs[0])\n",
    "\n",
    "                preds = 1 if probs > threshold else 0\n",
    "                cur_ans.append(preds)\n",
    "                cnt_1 = cur_ans.count(1)\n",
    "                cnt_0 = cur_ans.count(0)\n",
    "                ans = 1 if cnt_1 > cnt_0 else 0\n",
    "                answer_lst.append(ans)\n",
    "\n",
    "            self.submit[\"label\"] = answer_lst\n",
    "            self.submit[\"label\"] = self.submit[\"label\"].apply(lambda x: \"fake\" if x else \"real\")\n",
    "            self.submit.to_csv(\"sample_submissionv1.csv\", index=False)\n",
    "            \n",
    "            logit_df['label'] = logit_lst\n",
    "            logit_df.to_csv(\"sample_submission_logitv1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
